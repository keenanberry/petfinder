# petfinder
A web scraping project for my Unix Tools course. This tool will scrape [petfinder.com](https://www.petfinder.com/) for dogs in and around Austin, TX. The goal of the project is to generate a dataset of features associated with adoptable dogs. Long-term plans are to potentially add the data to Kaggle for others to use ML and NLP to predict/classify dogs with high probability of adoption.

# Introduction

The Unix operating system was developed in the mid-1970s and has quickly become one of the most widely used operating systems today. Due to its popularity, many versions of Unix exist, including a freely distributed version known as Linux. One of the greatest features of the Unix system was the development of the pipe, which allows programs to send their outputs directly to the next process for further analysis. Because the Unix tools are designed to operate on large streams of text data at fast speeds, they play an important role in the analysis of "big data" in fields such as data science, bioinformatics, and computational linguistics.

In this paper, I present a data generation pipeline that scrapes the web for characteristics of adoptable dogs, selecting features that may be of importance in determining the adoption success of a dog. The majority of this pipeline has been created with the standard GNU/Linux tool set, and the data comes from the site [petfinder.com](https://www.petfinder.com/), an online database of adoptable animals across North America. The goal of this work is to produce a rich data set of features that can be used in a machine learning analysis to predict a dog's chance of adoption. My hope is that subsequent analyses of this data will provide adoption agencies valuable insight so that they may tailor their databases and services to improve adoption success rates.

# Design

In accordance with the Unix philosophy of interoperability, the pipeline uses multiple Unix tools connected by pipes and the GNU Make tool to build the program from the \textit{makefile}. The program is freely available for download and use, and can be found at the project's [Github repository](https://github.com/keenanberry/petfinder) page. The repository contains a \textit{makefile} and a python script to scrape the JavaScript embedded web content found on \url{petfinder.com}.

In the \textit{makefile} the city, state, and date are defined as variables with the default settings configured to search for dogs in the Austin, TX area. However, the user can change the city name and state abbreviation if he/she desires to search the Petfinder database for dogs in other parts of the country. To collect the data, the user simply types `make` and the program begins by running *printSource.py*. This python script uses the Selenium library and ChromeDriver to download full web pages. Although the Unix `wget` command is typically used to download source code, this tool is not supported for use with embedded JavaScript. Thus, *printSource.py* is used to collect all pages associated with the search area (i.e. Austin, Tx). 

Next, a series of Unix tools `find | xargs | egrep | sed` are combined to output a shell script that uses the `wget` command to retrieve the source code for each dog profile found on all pages. Once the dog profiles are collected, the program uses a similar series of tools and the AWK scripting language to search each profile and output a 24 column text file which includes information corresponding to the following feature types: "primary_breed," "sex," "age," "primary_photo_url," "good_with_cats," "description," etc.

The output file is saved as `$(DATE)\_petfinder.txt`. The variable `DATE` acts as a unique identifier, so the program can be run a second time to determine the "label" for each dog: adopted or not-adopted. To run the program again, the user must type `make clean` to remove the web pages and dog profiles from the directory. Then, as long as the user types `make` on a different day, the program will re-execute, running the same processes to build the next `$(DATE)\_petfinder.txt` file. Once the *results* directory contains two files, the program uses the Unix `join` command to report all dogs from the original `$(DATE)\_petfinder.txt` file as "not-adopted" or "adopted" based on whether or not the dogs are found in the later `$(DATE)\_petfinder.txt` file. The resulting output is a tab-separated values (TSV) file with an additional column "label." One of the main limitations of this program is that the assignment of labels is determined by the assumption that dogs no longer in the database have been adopted; whereas in reality there are probably multiple reasons why a dog may no longer be listed on the Petfinder site.

# Results

To test the program, I ran \texttt{make} on 04-23-2019. Because of the sheer size of the Petfinder database available for Austin (a very dog friendly city), the program ran over night and by the end had downloaded over 100 main pages and 4000 dog profile pages (40 profiles/page). The program was unable to extract the dog data from a few hundred of the profile pages due to formatting issues with the regular expression, so the resulting file \textit{04-23-2019\_petfinder.txt} only contained 3932 rows, including a header row. 

Next I executed the program on 05-01-2019. Again, the program downloaded over 4000 dog pages from the web. This time the program was able to use the information from the second file \textit{05-01-2019\_petfinder.txt} to inform the assignment of labels for dogs in \textit{04-23-2019\_petfinder.txt}. The final \textit{petfinder.tsv} file contained information for 3931 dogs, 645 of which were labeled as "adopted." The results indicate that 16.4\% of the sample dog population was adopted after only one week.

Since the number of "not-adopted" labels was significantly greater than the "adopted" labels, it may be best for future tests to wait longer before executing the program the second time to ensure a more even distribution of labels. In addition, users may want to combine the data generated from the initial analysis with subsequent test results. Label distribution is an important aspect to consider when generating data sets for machine learning classification problems because non-uniform distributions can lead to biases in the model. However, if there is too much time between the first and second executions of the program, the assumptions made by the program may result in a greater number of incorrect labels. Thus, the user should be aware of these caveats before using this pipeline to generate data for his/her analysis.

After examining some of the features in the set of 645 "adopted" dogs, I found that of the three categories reported for "age" 364 were "Young"/"Baby" and 264 were "Adult." Similarly, I found among all types of "primary\_breeds" that "Labrador Retriever" dominated the "adopted" dog set at 131. This preliminary analysis suggests that certain characteristics may be more desirable for individuals seeking to adopt. Thus, this data generation program has the potential to provide valuable insights into the nature of dog adoptions.

# Conclusions

Although the resulting output data contains all the information that may be of importance in predicting adoption, there is still further processing that is needed before passing this data to a machine learning algorithm. Thus, future directions include further clean up of the data, such as removing unnecessary quotation marks from some of the columns. This task can be easily completed in the Unix environment or with the use of a programming language like Python. Additionally, since the regular expression used to pull the actual data from the dog profile pages failed for a few hundred of the pages, I plan to edit this expression (or use a JavaScript web scraping tool) to more effectively output all of the results. And lastly, I would like to eventually edit this program so that multiple 'final' data sets can be generated and eventually concatenated together. This will provide more training samples for machine learning purposes, and consequently, ensure a more robust classification model.

The results of this project clearly demonstrate the effectiveness of the Unix system tools in engineering data sets through web scraping and text manipulation processes. My hope is that this program and the data it generates can be used in downstream machine learning analyses to determine the impact specific features have on pet adoption. Specifically, I am less concerned about the physical characteristics associated with dogs and more focused on providing adoption agencies and rescue groups insight into the best practices for presenting their pet data to the public. 

In particular, I am interested to see how two features in this data set influence adoption success. The first is the "primary\_photo\_url" and "photo\_urls." In both of these columns, the URL links are provided, so anyone looking to run a computer vision analysis on these images can try to learn information about these photos and how they may contribute to the models prediction accuracy. The second is the "description" feature. This feature is highly variable in length and could be used in a natural language processing model to learn which words or phrases are most indicative of a successful adoption event.
